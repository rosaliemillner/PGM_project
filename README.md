# Are Generative Classifiers More Reliable for Medical Imaging? Insights from Adversarial and Non-Adversarial Perturbations
Final project for the 2024-2025 course: Probabilistic Graphical Models.
---

Please use `requirements.txt` or `environment.yml` to run the provided code.

With the increasing volume of medical images generated by various radiological imaging techniques, the use of AI assistance can significantly enhance clinical applications. However, even a minimal error, such as a one-pixel discrepancy in an image, or a change in brightness or contrast, can lead to incorrect predictions in medical image analysis. Such errors could result in misclassifications, which might lead to wrong clinical decisions. This vulnerability can be seen as adversarial/non-adversarial attacks on deep learning models. In this report, we explore the extent to which a deep generative classifier and a deep discriminative classifier are robust to perturbations, using a widely used MRI image multiclass dataset. Furthermore, experiments explore how altering medical images influences classification accuracy and the robustness of different DNN/GNN architectures. Results demonstrate that discriminative models for medical images are susceptible to these attacks, whereas GNN can be much more robust and thus lead to less class prediction errors.
